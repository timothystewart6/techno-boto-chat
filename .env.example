# Copy this file to .env.local and configure your settings
# LLM API Configuration
LLM_API_BASE_URL=http://localhost:12434/engines/v1
OPENAI_API_KEY= # Leave empty for local models like Docker Model Runner
MODEL_NAME=ai/gpt-oss
NEXT_TELEMETRY_DISABLED=1 # Disable Next.js telemetry
# Development settings (optional)
# NEXT_PUBLIC_DEBUG=true
# Enable reasoning support for compatible models (o1, Claude, gpt-oss, etc.)
ENABLE_REASONING=true
# Token limits for responses (optional - defaults to high limits)
NEXT_PUBLIC_MAX_TOKENS_REGULAR=1000
NEXT_PUBLIC_MAX_TOKENS_REASONING=2000
# System prompts (optional - defaults provided)
NEXT_PUBLIC_SYSTEM_PROMPT_REGULAR="You are a helpful AI assistant. Provide concise, friendly responses."
NEXT_PUBLIC_SYSTEM_PROMPT_REASONING="You are a helpful AI assistant. Think through the problem step by step, then provide a clear, concise final answer. Your reasoning will be shown separately from your final response."
